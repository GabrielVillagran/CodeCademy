{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyI8DDfQ6QtOwmt+Jv3ntk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielVillagran/CodeCademy/blob/main/Generating_Text_with_Deep_Learning_codecademy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text with Deep Learning\n",
        "\n",
        "Congrats! You’ve successfully built a machine translation program using deep learning with Tensorflow’s Keras API.\n",
        "\n",
        "While the translation output may not have been quite what you expected, this is just the beginning. There are many ways you can improve this program on your own device by using a larger data set, increasing the size of the model, and adding more epochs for training.\n",
        "\n",
        "You could also convert the one-hot vectors into word embeddings during training to improve the model. Using embeddings of words rather than one-hot vectors would help the model capture that semantically similar words might have semantically similar embeddings (helping the LSTM generalize).\n",
        "\n",
        "You’ve learned quite a bit, even beyond the Keras implementation:\n",
        "\n",
        "seq2seq models are deep learning models that use recurrent neural networks like LSTMs to generate output.\n",
        "In machine translation, seq2seq networks encompass two main parts:\n",
        "The encoder accepts language as input and outputs state vectors.\n",
        "The decoder accepts the encoder’s final state and outputs possible translations.\n",
        "Teacher forcing is a method we can use to train seq2seq decoders.\n",
        "We need to mark the beginning and end of target sentences so that the decoder knows what to expect at the beginning and end of sentences.\n",
        "One-hot vectors are a way to represent a given word in a set of words wherein we use 1 to indicate the current word and 0 to indicate every other word.\n",
        "Timesteps help us keep track of where we are in a sentence.\n",
        "We can adjust batch size, which determines how many sentences we give a model at a time.\n",
        "We can also tweak dimensionality and number of epochs, which can improve results with careful tuning.\n",
        "The Softmax function converts the output of the LSTMs into a probability distribution over words in our vocabulary."
      ],
      "metadata": {
        "id": "PTb2RoG190Xp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvoofTAx9y2B"
      },
      "outputs": [],
      "source": [
        "from training import encoder_inputs, decoder_inputs, encoder_states, decoder_lstm, decoder_dense, encoder_input_data, num_decoder_tokens\n",
        "\n",
        "from prep import target_features_dict, reverse_target_features_dict, max_decoder_seq_length, input_docs, target_docs, target_tokens\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model, load_model\n",
        "import numpy as np\n",
        "\n",
        "training_model = load_model('training_model.h5')\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "def decode_sequence(test_input):\n",
        "  encoder_states_value = encoder_model.predict(test_input)\n",
        "  decoder_states_value = encoder_states_value\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  stop_condition = False\n",
        "  while not stop_condition:\n",
        "    # Run the decoder model to get possible\n",
        "    # output tokens (with probabilities) & states\n",
        "\n",
        "\n",
        "    # Choose token with highest probability\n",
        "    sampled_token = \"\"\n",
        "\n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop token.\n",
        "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "\n",
        "\n",
        "    # Update states\n",
        "\n",
        "\n",
        "  return decoded_sentence\n",
        "\n",
        "for seq_index in range(10):\n",
        "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(test_input)\n",
        "  print('-')\n",
        "  print('Input sentence:', input_docs[seq_index])\n",
        "  print('Decoded sentence:', decoded_sentence)\n",
        "\n",
        ""
      ]
    }
  ]
}